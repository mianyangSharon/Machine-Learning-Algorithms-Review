{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Normalization\n",
    "Feature Scaling: make sure the features are on a similar scale\n",
    "There are two ways to achieve normalization:\n",
    "\n",
    "\n",
    "## Min-Max Scaling\n",
    "Using linear transformation to project the result to [0, 1]\n",
    "$$ X_{norm} = (X - X_{min}) / (X_{max} - X_{min}) $$\n",
    "\n",
    "\n",
    "\n",
    "## Z-Score Normalization\n",
    "$$ z = (x - mean)/std $$\n",
    "\n",
    "\n",
    "* Normalization will accelerate the gradient descent process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create some data to play with\n",
    "import random\n",
    "import numpy as np\n",
    "data = np.asarray([[random.randint(0,10)+ random.random() * i * j for i in range(10)] for j in range(10)], dtype = int)\n",
    "label = np.asarray([random.randint(0,1) for i in range(10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 6,  5,  8,  8,  2,  7,  6, 10,  4,  7],\n",
       "       [ 7,  2,  3,  9,  4,  7, 12,  8,  7,  1],\n",
       "       [ 1,  7,  9,  2, 11, 11, 12,  6,  7, 23],\n",
       "       [ 5, 10, 11, 11, 11, 14, 22, 22, 15,  9],\n",
       "       [ 8,  1, 12, 14, 10, 18, 27, 18, 22, 25],\n",
       "       [ 3,  6, 13, 12, 10, 22, 22,  4, 31, 17],\n",
       "       [ 3, 12, 16, 10, 16,  6, 30, 22, 34, 16],\n",
       "       [ 3,  9,  1, 20, 13, 18, 40,  2, 32, 37],\n",
       "       [ 7,  6, 18, 13,  4, 16, 54, 33, 62,  5],\n",
       "       [10, 13, 13, 30, 25, 12, 26,  8, 76, 50]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new matrix with scaled data using min-max scaling\n",
    "def min_max_scaling(data):\n",
    "    min_max = lambda v: (v - min(v))/(max(v) - min(v))\n",
    "    return np.apply_along_axis(min_max,0,data)\n",
    "\n",
    "#print(min_max_scaling(data))\n",
    "#print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def z_score_scaling(data):\n",
    "    z_score = lambda v: (v-np.mean(v))/np.std(v)\n",
    "    return np.apply_along_axis(z_score,0,data)\n",
    "\n",
    "#print(min_max_scaling(data))\n",
    "#print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Encoding Categorical Feature\n",
    "1. Ordinal Encoding\n",
    "2. One-hot Encoding\n",
    "3. Binary Encoding\n",
    "\n",
    "\n",
    "* algorithms like Logistic Regression need encoding categorical feature "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cold' 'cold' 'normal' 'normal' 'vert hot' 'very cold' 'normal' 'cold'\n",
      " 'hot' 'cold']\n"
     ]
    }
   ],
   "source": [
    "# create categorical features\n",
    "c1 = [\"very cold\", \"cold\", \"normal\", \"hot\", \"vert hot\"]\n",
    "c2 = [\"cotton\", \"silk\", \"wool\"]\n",
    "c3 = [\"orange\", \"apple\", \"banana\", \"lemon\", \"watermelon\", \"strawberry\", \"peach\", \"pear\"]\n",
    "data = np.asarray([[c1[random.randint(0,4)],c2[random.randint(0,2)],c3[random.randint(0,7)]] for j in range(10)])\n",
    "print(data[:,0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ordinal Encoding \n",
    "Ordinal Encoding is often used when the categories have an ordinal relationship.\n",
    "\n",
    "For example, grade \"High\", \"Mid\", \"Low\"\n",
    "\n",
    "Thus this method need to label each one by hand\n",
    "\n",
    "## One-hot Encoding\n",
    "One-hot encoding is used for categories that does not have a big/small difference\n",
    "\n",
    "Note this method could takes a lot of space with high dimensional data, to deal with it:\n",
    "\n",
    "1. use sparse vectors\n",
    "2. use feature selection techniques to lower the dimension\n",
    "\n",
    "## Binary Encoding\n",
    "Binary Encoding is like one-hot encoding, but takes less space. \n",
    "\n",
    "Step 1: Assign each categoy an ID (e.g. 1,2,3,4 ...)\n",
    "\n",
    "Step 2: (Hash Code the ID) Present the ID in binary number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 2 3 3 5 1 3 2 4 2]\n"
     ]
    }
   ],
   "source": [
    "# c1 uses ordinal encoding:\n",
    "def ordinal(value):\n",
    "    if value == \"very cold\":\n",
    "        return 1\n",
    "    elif value == \"cold\":\n",
    "        return 2\n",
    "    elif value ==\"normal\":\n",
    "        return 3\n",
    "    elif value ==\"hot\":\n",
    "        return 4\n",
    "    else:\n",
    "        return 5\n",
    "    \n",
    "def ordinal_encoding(col):\n",
    "    vfunc = np.vectorize(ordinal)\n",
    "    return vfunc(col)\n",
    "\n",
    "print(ordinal_encoding(data[:,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['wool' 'wool' 'silk' 'silk' 'silk' 'silk' 'cotton' 'silk' 'silk' 'silk']\n",
      "[[0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# c2 uses one-hot encoding\n",
    "def one_hot_encoding(col):\n",
    "    _dict={}\n",
    "    i = 0\n",
    "    for it in (set(col)):\n",
    "        _dict[it] = i\n",
    "        i += 1\n",
    "    d = np.zeros([len(col),len(set(col))])\n",
    "    for i in range(len(col)):\n",
    "        d[i, _dict[col[i]]]=1\n",
    "    return d\n",
    "\n",
    "print(data[:,1])\n",
    "print(one_hot_encoding(data[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['strawberry' 'pear' 'orange' 'orange' 'banana' 'orange' 'orange'\n",
      " 'watermelon' 'pear' 'banana']\n",
      "[100  10   0   0  11   0   0   1  10  11]\n"
     ]
    }
   ],
   "source": [
    "# c3 uses binary encoding\n",
    "\n",
    "# x is an non-negative integer\n",
    "def convert_to_binary(x):\n",
    "    if x < 2:\n",
    "        return x\n",
    "    ans = 0\n",
    "    i = 1\n",
    "    while x > 0:\n",
    "        ans += x%2 * i\n",
    "        i *= 10\n",
    "        x -= x%2\n",
    "        x/=2\n",
    "    return int(ans)\n",
    "\n",
    "#convert_to_binary(4) == 100\n",
    "\n",
    "def binary_encoding(col):\n",
    "    _dict={}\n",
    "    i = 0\n",
    "    for it in (set(col)):\n",
    "        _dict[it] = convert_to_binary(i)\n",
    "        i += 1\n",
    "    \n",
    "    f = lambda x: _dict[x]\n",
    "    vfunc = np.vectorize(f)\n",
    "    return vfunc(col)\n",
    "\n",
    "print(data[:,2])\n",
    "print(binary_encoding(data[:,2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
